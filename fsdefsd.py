import functions as f
from pyspark import SparkContext
sc = SparkContext()
vin = sc.textFile("vin.csv")
vout = sc.textFile("vout.csv")
vin = vin.map(lambda line: line.split(",")).filter(lambda list: len(list) == 3)
vin = vin.map(lambda list: ((list[1].strip(" "), list[2].strip(" ")), [list[0].strip(" ")]))
vout = vout.map(lambda line: line.split(",")).filter(lambda list: len(list) == 4)
vout = vout.map(lambda list: ((list[0].strip(" "), list[2].strip(" ")), [list[1].strip(" "), list[3].strip(" ")]))
result = vin.union(vout).reduceByKey(lambda a, b: a + b)
result = result.map(lambda (key, value): ((key[0]), [key[1]] + value))
result = result.filter(lambda (key, value): len(value) == 4)
wallets = result.map(lambda (key, value): (value[1], [value[3]]))
wallets = wallets.reduceByKey(lambda x, y: x + y)
wallets = wallets.map(lambda (key, value): (key, [value[0]]))
result2 = result.reduceByKey(lambda x, y: [x] + [y])
result3 = result2.union(wallets)
result3 = result3.reduceByKey(lambda x, y: [x] + [y])
result3.saveAsTextFile("result3")
result3 = result3.filter(lambda x: f.filter(x))
result3.saveAsTextFile("result4")
result4 = result3.map(lambda x: f.coolStory(x))
result4.saveAsTextFile("finalFINAL")
